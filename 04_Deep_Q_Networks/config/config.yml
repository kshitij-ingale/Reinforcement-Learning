Network:                            # Neural network parameters
    QNet:                           # Policy (or Actor) network parameters
        hidden_units: [32, 32]      # Number of hidden units (len(hidden_units) will be number of layers)
        normalization: null         # Normalizing layer outputs with batch norm [`batch`, `instance`, null], set null for no normalization
        activation: "leaky_relu"    # Activation function to be used (it should match with some function in tensorflow.nn module)
        learning_rate: 1e-2         # Learning rate for this network

Training:                           # Parameters for training agent
    batch_size: 1024                # Batch size for training Q-values network
    discount: 0.99                  # Discount factor for future returns
    test_episodes: 100              # Number of test episodes to be used for evaluating agent performance 
    test_frequency: 100             # Test learnt policy after every `test_frequency` episodes during training
    render_frequency: 100           # Render agent's interaction in environment after ever `render_frequency` episodes during training
    video_save_frequency: null      # Save videos at every `video_save_frequency` episodes out of test_episodes in evaluation step (during training), no video saved if set to null
    model_save_frequency: 100       # Save model weights after every `model_save_frequency` episodes during training
    update_target_frequency: 100    # Update target Q-net model after every `update_target_frequency` episodes
    update_smoothing: 1             # Smoothing for updating target Q-values network
    Exploration:
        intial_epsilon: 0.9         # Exploration probability at start of training
        final_epsilon: 0.1          # Exploration probability at end of training
        decay_steps: 50000          # Linear decay of exploration probability over `decay_steps` steps of agent
    Replay_memory:
        capacity: 50000             # Number of state-action transitions to be stored in replay memory
        burn_episodes: 10000        # Number of episodes to be used for burning in initial replay memory before training
    double: False                   # Use Double DQN if set to True
    duel: False                     # Use dueling networks if set to True

Inference:                          # Parameters for running inference
    video_save_frequency: 20        # Save videos at every `video_save_frequency` episodes out of test_episodes, no video saved if set to null

Directories:
    output: "output/"               # Directory to store output artifacts, logs, tensorboard files, videos, saved models
